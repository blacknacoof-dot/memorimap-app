import { GoogleGenerativeAI } from "@google/generative-ai";
import { Message } from "../types/consultation";
import { Facility } from "../types";
import { getMockAIResponse } from "./mockAI";

// Initialize Gemini Client
// NOTE: Ideally this should be server-side or via a proxy to protect the API key in production.
// For this MVP/Demo, client-side usage is acceptable with restrictions.
const API_KEY = import.meta.env.VITE_GOOGLE_GENAI_API_KEY || "AIzaSyDt2aQzcyigpeIZGWug1e-jE0raTxnFXUE";
const USE_REAL_AI = false; // [Mock Mode] Set to true to enable Gemini

let genAI: any = null;

try {
    if (API_KEY && USE_REAL_AI) {
        genAI = new GoogleGenerativeAI(API_KEY);
    }
} catch (e) {
    console.error("Failed to initialize Gemini Client", e);
}

export interface StreamResponse {
    text: string;
    isDone: boolean;
}

const SAFETY_SETTINGS = [
    {
        category: "HARM_CATEGORY_HARASSMENT",
        threshold: "BLOCK_MEDIUM_AND_ABOVE",
    },
    {
        category: "HARM_CATEGORY_HATE_SPEECH",
        threshold: "BLOCK_MEDIUM_AND_ABOVE",
    },
];

const MODEL_CONFIG = {
    model: "gemini-2.0-flash-exp",
    generationConfig: {
        temperature: 0.7,
        maxOutputTokens: 1024,
    },
    safetySettings: SAFETY_SETTINGS
};

export async function* streamConsultationMessage(
    facility: Facility,
    history: Message[],
    newMessage: string,
    topic: string,
    faqs: any[] = []
): AsyncGenerator<string, void, unknown> {
    // Try real API first, fallback to mock if it fails
    if (!USE_REAL_AI || !genAI || !API_KEY) {
        console.log("Using Mock AI (Simulation Mode)");
        yield* getMockAIResponse(facility, newMessage, topic);
        return;
    }

    // Retry initialization just in case
    if (!genAI) {
        try {
            // @ts-ignore
            const { GoogleGenerativeAI: GenAI } = await import("@google/generative-ai");
            genAI = new GenAI(API_KEY);
        } catch (e) {
            console.error("Re-init failed, using Mock AI", e);
            yield* getMockAIResponse(facility, newMessage, topic);
            return;
        }
    }

    const model = genAI.getGenerativeModel({
        model: MODEL_CONFIG.model,
        generationConfig: {
            ...MODEL_CONFIG.generationConfig,
            responseMimeType: "application/json" // Force JSON output
        }
    });

    // Construct System Prompt
    const systemPrompt = `
# Role: Facility AI Concierge (Urgent Direct Booking Mode)
You are 'ë§ˆìŒì´', the AI concierge for **${facility.name}**.

# Goal
ìœ ì¡±ì´ ì „í™” ìƒë‹´ ì—†ì´, ëª¨ë°”ì¼ ìƒì—ì„œ 'ì•ˆì¹˜ ì¼ì‹œ'ë¥¼ ì§ì ‘ ì§€ì •í•˜ê³  'ë°©ë¬¸ ì˜ˆì•½'ì„ ì™„ë£Œí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

# Interaction Guidelines
1. **No Phone Calls:** ì „í™” ì—°ê²°ì„ ê¶Œìœ í•˜ì§€ ë§ˆì„¸ìš”. ë°”ë¡œ ì‹œê°„ ì„ íƒ(Time Selection)ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.
2. **Direct Booking:** ì‚¬ìš©ìžê°€ ì‹œê°„ì„ ì„ íƒí•˜ë©´ ì¦‰ì‹œ DBì— ì˜ˆì•½ì„ í™•ì • ì§“ìŠµë‹ˆë‹¤.
3. **Compassionate Efficiency:** ìœ„ë¡œí•˜ë˜, ì ˆì°¨ëŠ” ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì•ˆë‚´í•©ë‹ˆë‹¤.
4. **JSON Output Only:** YOU MUST OUTPUT ONLY VALID JSON. No markdown backticks.

# Scenario Logic & Output Format

## Case 1: ê¸´ê¸‰ ì§„ìž… -> ë‚ ì§œ í™•ì¸
User: mode_urgent (ê¸´ê¸‰ ë²„íŠ¼ í´ë¦­) or "ê¸´ê¸‰" or "ìž¥ë¡€ ë°œìƒ"
AI Output:
{
  "message": "ì‚¼ê°€ ì¡°ì˜ë¥¼ í‘œí•©ë‹ˆë‹¤. ì „í™” ëŒ€ê¸° ì—†ì´ **ì§€ê¸ˆ ë°”ë¡œ ì•ˆì¹˜ ì˜ˆì•½**ì„ í™•ì •í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\\nì‹œì„¤ì— ë„ì°©í•˜ì‹œëŠ” ë‚ ì§œ(ë°œì¸ì¼)ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.",
  "options": [
    {"label": "ðŸ“… ì˜¤ëŠ˜ (ì¦‰ì‹œ ì´ë™)", "value": "date_today"},
    {"label": "ðŸ“… ë‚´ì¼", "value": "date_tomorrow"},
    {"label": "ðŸ“… ëª¨ë ˆ", "value": "date_dayafter"}
  ],
  "action_trigger": "URGENT_CHECK"
}

## Case 2: ìœ í˜• ì„ íƒ
User: date_tomorrow (or similar date selection)
AI Output:
{
  "message": "ë‚´ì¼ ì•ˆì¹˜ ê°€ëŠ¥í•œ ìžë¦¬ë¥¼ í™•ë³´í•˜ê² ìŠµë‹ˆë‹¤.\\nì–´ë–¤ ìœ í˜•ìœ¼ë¡œ ì¤€ë¹„í•´ ë“œë¦´ê¹Œìš”?",
  "options": [
    {"label": "ðŸ‘¤ ê°œì¸ë‹¨ (1ë¶„)", "value": "type_single"},
    {"label": "ðŸ‘¥ ë¶€ë¶€ë‹¨ (2ë¶„)", "value": "type_couple"}
  ],
  "action_trigger": "URGENT_CHECK"
}

## Case 3: ì‹œê°„ ì§€ì • (Time Picker)
User: type_single (or "ê°œì¸ë‹¨", "ë¶€ë¶€ë‹¨")
AI Output:
{
  "message": "ë„¤, ê°œì¸ë‹¨ ì—¬ìœ ë¶„ í™•ë³´ë˜ì—ˆìŠµë‹ˆë‹¤.\\në‚´ì¼ ë„ì°©í•˜ì…”ì„œ **ê³„ì•½ ë° ì•ˆì¹˜ë¥¼ ì§„í–‰í•  ì‹œê°„**ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.\\n(ì„ íƒí•˜ì‹  ì‹œê°„ì— ë§žì¶° ì§ì›ì´ ì„œë¥˜ë¥¼ ì¤€ë¹„í•˜ê³  ì •ë¬¸ì—ì„œ ëŒ€ê¸°í•©ë‹ˆë‹¤.)",
  "options": [
    {"label": "09:00 ë„ì°©", "value": "time_0900"},
    {"label": "11:00 ë„ì°©", "value": "time_1100"},
    {"label": "13:00 ë„ì°©", "value": "time_1300"},
    {"label": "15:00 ë„ì°©", "value": "time_1500"}
  ],
  "action_trigger": "URGENT_CHECK"
}

## Case 4: ì˜ˆì•½ í™•ì • (Final Action)
User: time_1100 (or any time selection)
AI Output:
{
  "message": "**[ì˜ˆì•½ í™•ì •] ë‚´ì¼ ì˜¤ì „ 11ì‹œ**ë¡œ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.\\në„ì°© ì¦‰ì‹œ ì•ˆì¹˜ê°€ ê°€ëŠ¥í•˜ë„ë¡ ì¤€ë¹„í•´ ë‘ê² ìŠµë‹ˆë‹¤.\\n\\nâš ï¸ **í•„ìˆ˜ ì§€ì°¸ ì„œë¥˜:**\\n1. í™”ìž¥ ì¦ëª…ì„œ\\n2. ê³„ì•½ìž ì‹ ë¶„ì¦\\n\\nì¡°ì‹¬ížˆ ì˜¤ì‹­ì‹œì˜¤.",
  "options": [
    {"label": "ðŸ“ ë‚´ë¹„ê²Œì´ì…˜ ì‹¤í–‰", "value": "open_navi"},
    {"label": "ðŸ“„ ì˜ˆì•½ì¦ ë³´ê¸° (ë°”ì½”ë“œ)", "value": "show_ticket"}
  ],
  "action_trigger": "URGENT_RESERVATION_CONFIRM" 
}

## Default / General Inquiry
For other queries, respond helpfully and suggest starting the urgent flow if appropriate.
Output structure must always range "message", "options" (optional), "action_trigger" (optional).

[ì‹œì„¤ ì •ë³´]
- ì´ë¦„: ${facility.name}
- ì£¼ì†Œ: ${facility.address}
- ê°€ê²©ëŒ€: ${facility.priceRange}
- ìƒì„¸ì„¤ëª…: ${facility.description}
- ê°€ê²©í‘œ: ${facility.prices.map(p => `${p.type}: ${p.price}`).join(', ')}

${faqs.length > 0 ? `
[ìžì£¼ ë¬»ëŠ” ì§ˆë¬¸(FAQ)]
${faqs.map((f, i) => `${i + 1}. Q: ${f.question}\n   A: ${f.answer}`).join('\n')}
` : ''}
`;

    // Transform history to Gemini format
    const chatHistory = history.map(msg => ({
        role: msg.role === 'user' ? 'user' : 'model',
        parts: [{ text: msg.text }]
    }));

    try {
        const chat = model.startChat({
            history: [
                {
                    role: "user",
                    parts: [{ text: systemPrompt }]
                },
                ...chatHistory
            ]
        });

        const result = await chat.sendMessageStream(newMessage);

        for await (const chunk of result.stream) {
            const chunkText = chunk.text();
            if (chunkText) {
                yield chunkText;
            }
        }
    } catch (error: any) {
        console.error("Gemini Streaming Error, falling back to Mock AI:", error);
        yield* getMockAIResponse(facility, newMessage, topic);
    }
}
